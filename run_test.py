#!/usr/bin/env python3
"""
è§†é¢‘ç›‘æ§æµ‹è¯•å·¥å…· - å•æ¨¡å‹æµ‹è¯• & å¤šæ¨¡å‹å¯¹æ¯”ï¼ˆå«æ€§èƒ½ç»Ÿè®¡ï¼‰
"""
import sys
import json
import time
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field, asdict
from typing import List, Optional

import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from config import VIDEO_DIR, OUTPUT_DIR, VLM_MODELS
from observation_layer import ObservationLayer
from utils.video_processor import VideoProcessor


# ==================== æ•°æ®ç»“æ„ ====================

@dataclass
class FrameResult:
    """å•å¸§ç»“æœ"""
    frame_id: int
    timestamp: float
    observation: str
    processing_time: float


@dataclass
class ModelResult:
    """å•ä¸ªæ¨¡å‹çš„æµ‹è¯•ç»“æœ"""
    model_key: str
    model_name: str
    load_time: float = 0.0
    vram_usage_gb: float = 0.0
    total_frames: int = 0
    total_time: float = 0.0
    avg_time_per_frame: float = 0.0
    min_time: float = 0.0
    max_time: float = 0.0
    frame_results: List[FrameResult] = field(default_factory=list)
    error: str = ""

    def to_dict(self):
        return {
            "model_key": self.model_key,
            "model_name": self.model_name,
            "load_time": round(self.load_time, 2),
            "vram_usage_gb": round(self.vram_usage_gb, 2),
            "total_frames": self.total_frames,
            "total_time": round(self.total_time, 2),
            "avg_time_per_frame": round(self.avg_time_per_frame, 2),
            "min_time": round(self.min_time, 2),
            "max_time": round(self.max_time, 2),
            "frame_results": [asdict(f) for f in self.frame_results],
            "error": self.error,
        }


@dataclass
class BenchmarkReport:
    """å®Œæ•´æµ‹è¯•æŠ¥å‘Š"""
    video_name: str
    video_duration: float
    sample_interval: float
    num_frames: int
    test_time: str
    gpu_name: str
    models: List[ModelResult] = field(default_factory=list)

    def to_dict(self):
        return {
            "video_name": self.video_name,
            "video_duration": round(self.video_duration, 2),
            "sample_interval": self.sample_interval,
            "num_frames": self.num_frames,
            "test_time": self.test_time,
            "gpu_name": self.gpu_name,
            "models": [m.to_dict() for m in self.models],
        }


# ==================== å·¥å…·å‡½æ•° ====================

def get_gpu_info() -> str:
    """è·å– GPU ä¿¡æ¯"""
    if torch.cuda.is_available():
        return torch.cuda.get_device_name(0)
    return "CPU"


def get_vram_usage() -> float:
    """è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨é‡ (GB)"""
    if torch.cuda.is_available():
        return torch.cuda.memory_allocated() / 1024**3
    return 0.0


def list_test_videos():
    """åˆ—å‡ºæ‰€æœ‰æµ‹è¯•è§†é¢‘"""
    print(f"\næµ‹è¯•è§†é¢‘ç›®å½•: {VIDEO_DIR}")

    if not VIDEO_DIR.exists():
        print("ç›®å½•ä¸å­˜åœ¨!")
        return []

    videos = sorted(VIDEO_DIR.glob("*"))
    video_extensions = {'.mp4', '.avi', '.mkv', '.mov', '.webm'}
    valid_videos = [v for v in videos if v.suffix.lower() in video_extensions]

    print(f"æ‰¾åˆ° {len(valid_videos)} ä¸ªè§†é¢‘æ–‡ä»¶:\n")
    for i, v in enumerate(valid_videos, 1):
        print(f"  [{i}] {v.name}")

    return valid_videos


def select_video(videos):
    """é€‰æ‹©è¦æµ‹è¯•çš„è§†é¢‘"""
    while True:
        try:
            choice = input("\nè¯·é€‰æ‹©è§†é¢‘ç¼–å· (æˆ–è¾“å…¥ 'q' é€€å‡º): ").strip()
            if choice.lower() == 'q':
                return None
            idx = int(choice) - 1
            if 0 <= idx < len(videos):
                return videos[idx]
            print("æ— æ•ˆç¼–å·ï¼Œè¯·é‡è¯•")
        except ValueError:
            print("è¯·è¾“å…¥æ•°å­—")


def select_model():
    """é€‰æ‹©å•ä¸ªæ¨¡å‹"""
    models = list(VLM_MODELS.items())

    print("\nå¯ç”¨æ¨¡å‹:")
    for i, (key, info) in enumerate(models, 1):
        print(f"  [{i}] {key} - {info['description']}")

    while True:
        try:
            choice = input("\nè¯·é€‰æ‹©æ¨¡å‹ç¼–å·: ").strip()
            idx = int(choice) - 1
            if 0 <= idx < len(models):
                return models[idx][0]
            print("æ— æ•ˆç¼–å·ï¼Œè¯·é‡è¯•")
        except ValueError:
            print("è¯·è¾“å…¥æ•°å­—")


def select_models():
    """é€‰æ‹©å¤šä¸ªæ¨¡å‹"""
    models = list(VLM_MODELS.items())

    print("\nå¯ç”¨æ¨¡å‹ (è¾“å…¥ç¼–å·ï¼Œç©ºæ ¼åˆ†éš”ï¼Œæˆ– 'A' å…¨é€‰):")
    for i, (key, info) in enumerate(models, 1):
        print(f"  [{i}] {key} - {info['description']}")

    choice = input("\né€‰æ‹©: ").strip().upper()

    if choice == 'A':
        return [key for key, _ in models]

    selected = []
    for c in choice.split():
        try:
            idx = int(c) - 1
            if 0 <= idx < len(models):
                selected.append(models[idx][0])
        except ValueError:
            pass

    return selected if selected else [models[0][0]]


def select_interval():
    """é€‰æ‹©æŠ½å¸§é—´éš”"""
    print("\næŠ½å¸§é—´éš”é€‰é¡¹:")
    print("  [1] 1ç§’ (ç»†è‡´åˆ†æ)")
    print("  [2] 2ç§’ (æ¨è)")
    print("  [3] 5ç§’ (å¿«é€Ÿæµè§ˆ)")
    print("  [4] è‡ªå®šä¹‰")
    print("  [0] æ¯ä¸€å¸§ (æé™æµ‹è¯•ï¼Œéå¸¸è€—æ—¶!)")

    while True:
        try:
            choice = input("\nè¯·é€‰æ‹© (é»˜è®¤2): ").strip() or "2"
            if choice == "0":
                print("âš ï¸  è­¦å‘Šï¼šæ¯å¸§éƒ½å¤„ç†ä¼šéå¸¸è€—æ—¶ï¼")
                return 0.033  # çº¦30FPS
            elif choice == "1":
                return 1.0
            elif choice == "2":
                return 2.0
            elif choice == "3":
                return 5.0
            elif choice == "4":
                return float(input("è¾“å…¥é—´éš”ç§’æ•°: "))
            print("æ— æ•ˆé€‰æ‹©")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")


# ==================== æ‰“å°å‡½æ•° ====================

def print_comparison_table(report: BenchmarkReport):
    """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""
    print("\n" + "=" * 95)
    print("  æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("=" * 95)
    print(f"  è§†é¢‘: {report.video_name} | å¸§æ•°: {report.num_frames} | GPU: {report.gpu_name}")
    print("=" * 95)

    # è¡¨å¤´
    print(f"\n{'æ¨¡å‹':<18} {'åŠ è½½æ—¶é—´':>10} {'æ˜¾å­˜':>10} {'å¹³å‡/å¸§':>10} {'æœ€å¿«':>10} {'æœ€æ…¢':>10} {'çŠ¶æ€':<10}")
    print("-" * 95)

    for m in report.models:
        if m.error:
            status = f"âŒ {m.error[:15]}"
            print(f"{m.model_key:<18} {'-':>10} {'-':>10} {'-':>10} {'-':>10} {'-':>10} {status}")
        else:
            status = "âœ…"
            print(f"{m.model_key:<18} {m.load_time:>8.2f}s {m.vram_usage_gb:>8.2f}GB "
                  f"{m.avg_time_per_frame:>8.2f}s {m.min_time:>8.2f}s {m.max_time:>8.2f}s {status}")

    print("-" * 95)

    # æ‰¾å‡ºæœ€å¿«çš„æ¨¡å‹
    successful = [m for m in report.models if not m.error]
    if successful:
        fastest = min(successful, key=lambda x: x.avg_time_per_frame)
        smallest_vram = min(successful, key=lambda x: x.vram_usage_gb)
        print(f"\nğŸ† æœ€å¿«: {fastest.model_key} ({fastest.avg_time_per_frame:.2f}s/å¸§)")
        print(f"ğŸ’¾ æœ€çœæ˜¾å­˜: {smallest_vram.model_key} ({smallest_vram.vram_usage_gb:.2f}GB)")


def print_observation_comparison(report: BenchmarkReport, frame_id: int = 0):
    """æ‰“å°æŒ‡å®šå¸§çš„è§‚å¯Ÿå¯¹æ¯”"""
    print(f"\n{'=' * 95}")
    print(f"  å¸§ {frame_id} è§‚å¯Ÿç»“æœå¯¹æ¯”")
    print("=" * 95)

    for m in report.models:
        if m.error:
            continue

        frame = next((f for f in m.frame_results if f.frame_id == frame_id), None)
        if frame:
            print(f"\nã€{m.model_key}ã€‘(è€—æ—¶: {frame.processing_time:.2f}s)")
            print("-" * 50)
            # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
            obs = frame.observation
            if len(obs) > 500:
                obs = obs[:500] + "..."
            print(obs)


def save_report(report: BenchmarkReport) -> str:
    """ä¿å­˜æŠ¥å‘Šåˆ° JSON"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = OUTPUT_DIR / f"benchmark_{report.video_name}_{timestamp}.json"

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(report.to_dict(), f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    return str(output_path)


# ==================== æµ‹è¯•å‡½æ•° ====================

def run_single_model_test():
    """å•æ¨¡å‹æµ‹è¯•"""
    videos = list_test_videos()
    if not videos:
        return

    video = select_video(videos)
    if not video:
        return

    model_key = select_model()
    interval = select_interval()

    max_frames_input = input("\næœ€å¤§å¤„ç†å¸§æ•° (ç›´æ¥å›è½¦å¤„ç†å…¨éƒ¨): ").strip()
    max_frames = int(max_frames_input) if max_frames_input else None

    print(f"\n{'='*60}")
    print(f"å¼€å§‹æµ‹è¯•")
    print(f"  è§†é¢‘: {video.name}")
    print(f"  æ¨¡å‹: {model_key}")
    print(f"  é—´éš”: {interval}ç§’")
    print(f"  æœ€å¤§å¸§æ•°: {max_frames or 'å…¨éƒ¨'}")
    print(f"{'='*60}")

    confirm = input("\nç¡®è®¤å¼€å§‹? (y/n): ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        return

    with ObservationLayer(model_key=model_key) as layer:
        result = layer.process_video(
            video_path=str(video),
            sample_interval=interval,
            max_frames=max_frames,
        )
        layer.export_results(result.video_name)
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_DIR}")


def run_model_comparison():
    """å¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•ï¼ˆå¸¦æ€§èƒ½ç»Ÿè®¡ï¼‰"""
    videos = list_test_videos()
    if not videos:
        return

    video = select_video(videos)
    if not video:
        return

    selected_models = select_models()
    if len(selected_models) < 1:
        print("è¯·è‡³å°‘é€‰æ‹©1ä¸ªæ¨¡å‹")
        return

    interval = select_interval()

    max_frames_input = input("\næµ‹è¯•å¸§æ•° (ç›´æ¥å›è½¦=å¤„ç†å…¨éƒ¨ï¼Œè¾“å…¥æ•°å­—=é™åˆ¶å¸§æ•°): ").strip()
    max_frames = int(max_frames_input) if max_frames_input else None

    print(f"\n{'='*60}")
    print(f"å¼€å§‹å¤šæ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print(f"  è§†é¢‘: {video.name}")
    print(f"  æ¨¡å‹: {', '.join(selected_models)}")
    print(f"  é—´éš”: {interval}ç§’")
    print(f"  å¸§æ•°: {max_frames}")
    print(f"  GPU: {get_gpu_info()}")
    print(f"{'='*60}")

    confirm = input("\nç¡®è®¤å¼€å§‹? (y/n): ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        return

    # è·å–è§†é¢‘ä¿¡æ¯
    processor = VideoProcessor(str(video))
    video_duration = processor.video_info.duration
    processor.close()

    # åˆå§‹åŒ–æŠ¥å‘Š
    report = BenchmarkReport(
        video_name=video.name,
        video_duration=video_duration,
        sample_interval=interval,
        num_frames=max_frames,
        test_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        gpu_name=get_gpu_info(),
    )

    # é€ä¸ªæµ‹è¯•æ¨¡å‹
    for model_idx, model_key in enumerate(selected_models):
        print(f"\n{'#' * 70}")
        print(f"# [{model_idx + 1}/{len(selected_models)}] æµ‹è¯•æ¨¡å‹: {model_key}")
        print(f"{'#' * 70}")

        model_info = VLM_MODELS.get(model_key, {})
        result = ModelResult(
            model_key=model_key,
            model_name=model_info.get("name", model_key),
        )

        try:
            # æ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
            if torch.cuda.is_available():
                torch.cuda.synchronize()

            # åˆ›å»ºè§‚å¯Ÿå±‚
            layer = ObservationLayer(model_key=None)

            # åŠ è½½æ¨¡å‹å¹¶è®¡æ—¶
            load_start = time.time()
            success = layer.load_model(model_key)
            result.load_time = time.time() - load_start

            if not success:
                result.error = "åŠ è½½å¤±è´¥"
                report.models.append(result)
                layer.close()
                continue

            result.vram_usage_gb = get_vram_usage()
            print(f"æ¨¡å‹åŠ è½½: {result.load_time:.2f}ç§’, æ˜¾å­˜: {result.vram_usage_gb:.2f}GB")

            # å¤„ç†è§†é¢‘
            processor = VideoProcessor(str(video))
            frame_times = []
            total_start = time.time()

            for frame_info in processor.extract_frames(interval=interval, max_frames=max_frames):
                frame_start = time.time()

                try:
                    observation = layer.vlm.generate(
                        images=frame_info.image,
                        prompt="è¯·ä»”ç»†è§‚å¯Ÿè¿™å¼ å›¾ç‰‡ï¼Œè¯¦ç»†æè¿°ï¼šåœºæ™¯ã€äººç‰©ã€åŠ¨ä½œã€ç‰©ä½“ã€‚ç”¨è‡ªç„¶æµç•…çš„ä¸­æ–‡ã€‚",
                        max_new_tokens=256,
                        temperature=0.7,
                    )
                except Exception as e:
                    observation = f"[é”™è¯¯: {e}]"

                frame_time = time.time() - frame_start
                frame_times.append(frame_time)

                result.frame_results.append(FrameResult(
                    frame_id=frame_info.frame_id,
                    timestamp=frame_info.timestamp,
                    observation=observation,
                    processing_time=frame_time,
                ))

                ts = f"{int(frame_info.timestamp // 60):02d}:{int(frame_info.timestamp % 60):02d}"
                print(f"  [å¸§ {frame_info.frame_id}] {ts} - {frame_time:.2f}s")

            processor.close()

            # ç»Ÿè®¡
            result.total_frames = len(frame_times)
            result.total_time = time.time() - total_start
            result.avg_time_per_frame = sum(frame_times) / len(frame_times) if frame_times else 0
            result.min_time = min(frame_times) if frame_times else 0
            result.max_time = max(frame_times) if frame_times else 0

            print(f"\n  ç»Ÿè®¡: å¹³å‡ {result.avg_time_per_frame:.2f}s/å¸§, "
                  f"æ€»è®¡ {result.total_time:.2f}s")

            layer.close()

        except Exception as e:
            result.error = str(e)
            print(f"æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

        report.models.append(result)
        torch.cuda.empty_cache()

    # æ‰“å°ç»“æœ
    print_comparison_table(report)
    print_observation_comparison(report, frame_id=0)

    # ä¿å­˜æŠ¥å‘Š
    save_report(report)


# ==================== å¤šå¸§æµ‹è¯• ====================

def run_multi_frame_test():
    """å¤šå¸§è¾“å…¥æµ‹è¯• - éªŒè¯åŠ¨æ€åŠ¨ä½œè¯†åˆ«"""
    videos = list_test_videos()
    if not videos:
        return

    video = select_video(videos)
    if not video:
        return

    model_key = select_model()

    print("\nå¤šå¸§æµ‹è¯•é…ç½®:")
    print("  å°†è¿ç»­Nå¸§ä¸€èµ·é€å…¥VLMï¼Œæµ‹è¯•èƒ½å¦è¯†åˆ«åŠ¨æ€åŠ¨ä½œ")

    # é…ç½®å‚æ•°
    num_frames = int(input("\nè¿ç»­å¸§æ•° (æ¨è3-5): ").strip() or "5")
    frame_interval = float(input("å¸§é—´éš”ç§’æ•° (æ¨è0.5-1): ").strip() or "0.5")
    start_time = float(input("èµ·å§‹æ—¶é—´ç§’ (é»˜è®¤0): ").strip() or "0")

    print(f"\n{'='*60}")
    print(f"å¤šå¸§åŠ¨æ€è¯†åˆ«æµ‹è¯•")
    print(f"  è§†é¢‘: {video.name}")
    print(f"  æ¨¡å‹: {model_key}")
    print(f"  å¸§æ•°: {num_frames} å¸§")
    print(f"  é—´éš”: {frame_interval} ç§’")
    print(f"  èŒƒå›´: {start_time}s ~ {start_time + num_frames * frame_interval}s")
    print(f"{'='*60}")

    confirm = input("\nç¡®è®¤å¼€å§‹? (y/n): ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        return

    # æå–è¿ç»­å¸§
    processor = VideoProcessor(str(video))
    frames = []

    print(f"\næå– {num_frames} å¸§...")
    for i in range(num_frames):
        timestamp = start_time + i * frame_interval
        frame_info = processor.get_frame_at(timestamp)
        if frame_info:
            frames.append(frame_info.image)
            print(f"  å¸§ {i+1}: {timestamp:.1f}s âœ“")
        else:
            print(f"  å¸§ {i+1}: {timestamp:.1f}s âœ— (è¶…å‡ºè§†é¢‘èŒƒå›´)")

    processor.close()

    if len(frames) < 2:
        print("å¸§æ•°ä¸è¶³ï¼Œæ— æ³•æµ‹è¯•")
        return

    # åŠ è½½æ¨¡å‹
    print(f"\nåŠ è½½æ¨¡å‹ {model_key}...")
    from models.vlm_loader import VLMLoader
    vlm = VLMLoader()

    if not vlm.load_model(model_key):
        print("æ¨¡å‹åŠ è½½å¤±è´¥")
        return

    # å¤šå¸§ prompt
    multi_frame_prompt = """è¿™æ˜¯è¿ç»­çš„è§†é¢‘å¸§æˆªå›¾ï¼Œè¯·è§‚å¯Ÿè¿™äº›å›¾ç‰‡çš„å˜åŒ–ï¼Œå›ç­”ï¼š
1. åœºæ™¯æè¿°ï¼šè¿™æ˜¯ä»€ä¹ˆåœ°æ–¹ï¼Ÿ
2. äººç‰©åŠ¨ä½œï¼šäººç‰©æ­£åœ¨åšä»€ä¹ˆåŠ¨ä½œ/æ´»åŠ¨ï¼Ÿï¼ˆæ³¨æ„è§‚å¯Ÿå§¿åŠ¿å˜åŒ–ï¼‰
3. åŠ¨ä½œåˆ¤æ–­ï¼šè¿™æ˜¯é™æ­¢çš„è¿˜æ˜¯åŠ¨æ€çš„æ´»åŠ¨ï¼Ÿå¦‚æœæ˜¯åŠ¨æ€çš„ï¼Œå…·ä½“æ˜¯ä»€ä¹ˆæ´»åŠ¨ï¼ˆå¦‚è·³èˆã€èµ°è·¯ã€è¿åŠ¨ç­‰ï¼‰ï¼Ÿ
è¯·ç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ã€‚"""

    # å•å¸§å¯¹æ¯” prompt
    single_frame_prompt = "è¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„åœºæ™¯ã€äººç‰©å’ŒåŠ¨ä½œã€‚"

    print(f"\n{'='*60}")
    print("æµ‹è¯•ç»“æœå¯¹æ¯”")
    print(f"{'='*60}")

    # æµ‹è¯•1: å•å¸§ï¼ˆç¬¬ä¸€å¸§ï¼‰
    print("\nã€å•å¸§æµ‹è¯•ã€‘ï¼ˆåªçœ‹ç¬¬ä¸€å¸§ï¼‰")
    print("-" * 50)
    start = time.time()
    single_result = vlm.generate(
        images=frames[0],
        prompt=single_frame_prompt,
        max_new_tokens=256,
        temperature=0.7,
    )
    single_time = time.time() - start
    print(f"è€—æ—¶: {single_time:.2f}s")
    print(single_result)

    # æµ‹è¯•2: å¤šå¸§
    print(f"\nã€å¤šå¸§æµ‹è¯•ã€‘ï¼ˆ{len(frames)}å¸§è¿ç»­ï¼‰")
    print("-" * 50)
    start = time.time()
    multi_result = vlm.generate(
        images=frames,
        prompt=multi_frame_prompt,
        max_new_tokens=512,
        temperature=0.7,
    )
    multi_time = time.time() - start
    print(f"è€—æ—¶: {multi_time:.2f}s")
    print(multi_result)

    # å¯¹æ¯”
    print(f"\n{'='*60}")
    print("æ€§èƒ½å¯¹æ¯”")
    print(f"{'='*60}")
    print(f"  å•å¸§: {single_time:.2f}s")
    print(f"  å¤šå¸§: {multi_time:.2f}s ({len(frames)}å¸§)")
    print(f"  å¤šå¸§/å•å¸§: {multi_time/single_time:.1f}x")

    vram = get_vram_usage()
    print(f"  æ˜¾å­˜å ç”¨: {vram:.2f}GB")

    vlm.unload_model()

    # ä¿å­˜ç»“æœ
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = OUTPUT_DIR / f"multiframe_test_{video.name}_{timestamp_str}.json"

    result = {
        "video": video.name,
        "model": model_key,
        "num_frames": len(frames),
        "frame_interval": frame_interval,
        "start_time": start_time,
        "single_frame": {
            "result": single_result,
            "time": single_time,
        },
        "multi_frame": {
            "result": multi_result,
            "time": multi_time,
        },
        "vram_gb": vram,
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜: {output_path}")


# ==================== è§†é¢‘æ¨¡å¼æµ‹è¯• ====================

def run_video_mode_test():
    """Qwen2-VL åŸç”Ÿè§†é¢‘æ¨¡å¼æµ‹è¯•"""
    videos = list_test_videos()
    if not videos:
        return

    video = select_video(videos)
    if not video:
        return

    print("\n" + "=" * 60)
    print("  Qwen2-VL è§†é¢‘æ¨¡å¼æµ‹è¯•")
    print("=" * 60)
    print("\næ­¤æµ‹è¯•ä½¿ç”¨ Qwen2-VL çš„åŸç”Ÿè§†é¢‘è¾“å…¥åŠŸèƒ½")
    print("ç›´æ¥å°†è§†é¢‘æ–‡ä»¶é€å…¥æ¨¡å‹ï¼Œè€ŒéæŠ½å¸§")

    # é…ç½®å‚æ•°
    print("\nè§†é¢‘å‚æ•°é…ç½®:")
    print("  æ³¨æ„ï¼šè§†é¢‘æ¨¡å¼æ˜¾å­˜å ç”¨å¤§ï¼Œå»ºè®®é™åˆ¶å‚æ•°")
    print("  é‡‡æ ·æ¨¡å¼ï¼šæŒ‡å®šæ€»å¸§æ•°ï¼Œä»è§†é¢‘ä¸­å‡åŒ€é‡‡æ ·")
    max_frames = int(input("  æ€»å¸§æ•° (æ¨è4-8ï¼Œé»˜è®¤4): ").strip() or "4")
    # min_pixels é»˜è®¤æ˜¯ 256*28*28ï¼Œæ‰€ä»¥åˆ†è¾¨ç‡è‡³å°‘éœ€è¦ 336
    resolution = int(input("  åˆ†è¾¨ç‡ (æœ€å°336ï¼Œæ¨è336-480ï¼Œé»˜è®¤336): ").strip() or "336")

    print(f"\n{'='*60}")
    print(f"æµ‹è¯•é…ç½®")
    print(f"  è§†é¢‘: {video.name}")
    print(f"  æ¨¡å‹: Qwen2-VL-2B (è§†é¢‘æ¨¡å¼)")
    print(f"  é‡‡æ ·å¸§æ•°: {max_frames} å¸§ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰")
    print(f"  åˆ†è¾¨ç‡: {resolution}x{resolution}")
    print(f"{'='*60}")

    confirm = input("\nç¡®è®¤å¼€å§‹? (y/n): ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        return

    # å…ˆæ¸…ç†å¯èƒ½æ®‹ç•™çš„æ˜¾å­˜
    import gc
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    print(f"\næ¸…ç†åæ˜¾å­˜: {get_vram_usage():.2f}GB")

    # åŠ è½½æ¨¡å‹
    print("\næ­£åœ¨åŠ è½½ Qwen2-VL-2B...")
    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
    from qwen_vl_utils import process_vision_info

    model_name = "Qwen/Qwen2-VL-2B-Instruct"
    model = None
    processor = None

    load_start = time.time()

    # æ£€æŸ¥ Flash Attention
    model_kwargs = {
        "torch_dtype": torch.bfloat16,
        "device_map": "auto",
    }
    try:
        import flash_attn
        model_kwargs["attn_implementation"] = "flash_attention_2"
        print("ä½¿ç”¨ Flash Attention 2")
    except ImportError:
        print("Flash Attention æœªå®‰è£…ï¼Œä½¿ç”¨é»˜è®¤ attention")

    model = Qwen2VLForConditionalGeneration.from_pretrained(model_name, **model_kwargs)
    processor = AutoProcessor.from_pretrained(model_name)

    load_time = time.time() - load_start
    print(f"æ¨¡å‹åŠ è½½å®Œæˆ: {load_time:.2f}s")

    vram = get_vram_usage()
    print(f"æ˜¾å­˜ä½¿ç”¨: {vram:.2f}GB")

    # æ„å»ºè§†é¢‘è¾“å…¥æ¶ˆæ¯
    # nframes: ä»è§†é¢‘ä¸­å‡åŒ€é‡‡æ ·çš„å¸§æ•°
    # max_pixels: æ¯å¸§æœ€å¤§åƒç´ æ•°ï¼ˆæ§åˆ¶åˆ†è¾¨ç‡ï¼‰
    video_config = {
        "type": "video",
        "video": str(video),
        "nframes": max_frames,
        "max_pixels": resolution * resolution,
    }

    result1 = ""
    result2 = ""
    infer_time1 = 0
    infer_time2 = 0

    try:
        # æµ‹è¯•1: åœºæ™¯å’ŒåŠ¨ä½œæè¿°
        print("\n" + "=" * 60)
        print("æµ‹è¯•1: è§†é¢‘å†…å®¹æè¿°")
        print("-" * 60)

        messages1 = [{
            "role": "user",
            "content": [
                video_config,
                {"type": "text", "text": "è¯·è§‚çœ‹è¿™æ®µè§†é¢‘ï¼Œè¯¦ç»†æè¿°ï¼š\n1. è§†é¢‘åœºæ™¯\n2. å‡ºç°çš„äººç‰©\n3. äººç‰©æ­£åœ¨è¿›è¡Œä»€ä¹ˆæ´»åŠ¨/åŠ¨ä½œ\nç”¨ä¸­æ–‡å›ç­”ã€‚"}
            ]
        }]

        text1 = processor.apply_chat_template(messages1, tokenize=False, add_generation_prompt=True)
        image_inputs1, video_inputs1 = process_vision_info(messages1)

        inputs1 = processor(
            text=[text1],
            images=image_inputs1,
            videos=video_inputs1,
            padding=True,
            return_tensors="pt",
        ).to(model.device)

        infer_start = time.time()
        with torch.no_grad():
            generated_ids1 = model.generate(
                **inputs1,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                repetition_penalty=1.2,
            )
        infer_time1 = time.time() - infer_start

        generated_ids_trimmed1 = [
            out_ids[len(in_ids):]
            for in_ids, out_ids in zip(inputs1.input_ids, generated_ids1)
        ]
        result1 = processor.batch_decode(generated_ids_trimmed1, skip_special_tokens=True)[0]

        print(f"è€—æ—¶: {infer_time1:.2f}s")
        print(f"\n{result1}")

        # æ¸…ç†ä¸­é—´å˜é‡
        del inputs1, generated_ids1, image_inputs1, video_inputs1
        torch.cuda.empty_cache()

        # æµ‹è¯•2: åŠ¨ä½œè¯†åˆ«ä¸“é¡¹
        print("\n" + "=" * 60)
        print("æµ‹è¯•2: åŠ¨ä½œè¯†åˆ«ä¸“é¡¹")
        print("-" * 60)

        messages2 = [{
            "role": "user",
            "content": [
                video_config,
                {"type": "text", "text": "è¯·ä»”ç»†è§‚å¯Ÿè§†é¢‘ä¸­äººç‰©çš„åŠ¨ä½œï¼Œåˆ¤æ–­ï¼š\n1. è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„æ´»åŠ¨ï¼Ÿï¼ˆå¦‚ï¼šè·³èˆã€è¿åŠ¨ã€åšé¥­ã€å·¥ä½œã€ä¼‘æ¯ç­‰ï¼‰\n2. åŠ¨ä½œæ˜¯é™æ€çš„è¿˜æ˜¯åŠ¨æ€è¿ç»­çš„ï¼Ÿ\n3. å¦‚æœæ˜¯åŠ¨æ€çš„ï¼Œæè¿°åŠ¨ä½œçš„ç‰¹ç‚¹ã€‚\nç›´æ¥ç»™å‡ºåˆ¤æ–­ç»“æœã€‚"}
            ]
        }]

        text2 = processor.apply_chat_template(messages2, tokenize=False, add_generation_prompt=True)
        image_inputs2, video_inputs2 = process_vision_info(messages2)

        inputs2 = processor(
            text=[text2],
            images=image_inputs2,
            videos=video_inputs2,
            padding=True,
            return_tensors="pt",
        ).to(model.device)

        infer_start = time.time()
        with torch.no_grad():
            generated_ids2 = model.generate(
                **inputs2,
                max_new_tokens=256,
                temperature=0.5,
                do_sample=True,
                repetition_penalty=1.2,
            )
        infer_time2 = time.time() - infer_start

        generated_ids_trimmed2 = [
            out_ids[len(in_ids):]
            for in_ids, out_ids in zip(inputs2.input_ids, generated_ids2)
        ]
        result2 = processor.batch_decode(generated_ids_trimmed2, skip_special_tokens=True)[0]

        print(f"è€—æ—¶: {infer_time2:.2f}s")
        print(f"\n{result2}")

        # æ€§èƒ½ç»Ÿè®¡
        print("\n" + "=" * 60)
        print("æ€§èƒ½ç»Ÿè®¡")
        print("=" * 60)
        print(f"  æ¨¡å‹åŠ è½½: {load_time:.2f}s")
        print(f"  æµ‹è¯•1è€—æ—¶: {infer_time1:.2f}s")
        print(f"  æµ‹è¯•2è€—æ—¶: {infer_time2:.2f}s")
        print(f"  æ€»æ¨ç†æ—¶é—´: {infer_time1 + infer_time2:.2f}s")
        print(f"  æ˜¾å­˜å ç”¨: {get_vram_usage():.2f}GB")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # ç¡®ä¿æ¸…ç†æ˜¾å­˜
        print("\næ­£åœ¨æ¸…ç†æ˜¾å­˜...")
        if model is not None:
            del model
        if processor is not None:
            del processor
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        print(f"æ¸…ç†å®Œæˆï¼Œå½“å‰æ˜¾å­˜: {get_vram_usage():.2f}GB")

    # ä¿å­˜ç»“æœ
    if result1 or result2:
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = OUTPUT_DIR / f"video_mode_test_{video.name}_{timestamp_str}.json"

        result = {
            "video": video.name,
            "model": "qwen2-vl-2b",
            "mode": "video",
            "nframes": max_frames,
            "resolution": resolution,
            "load_time": load_time,
            "test1": {
                "prompt": "åœºæ™¯å’ŒåŠ¨ä½œæè¿°",
                "result": result1,
                "time": infer_time1,
            },
            "test2": {
                "prompt": "åŠ¨ä½œè¯†åˆ«ä¸“é¡¹",
                "result": result2,
                "time": infer_time2,
            },
            "vram_gb": vram,
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜: {output_path}")


# ==================== LLaVA-NeXT-Video æµ‹è¯• ====================

def run_llava_next_video_test():
    """LLaVA-NeXT-Video ä¸“ç”¨è§†é¢‘æ¨¡å‹æµ‹è¯•"""
    videos = list_test_videos()
    if not videos:
        return

    video = select_video(videos)
    if not video:
        return

    print("\n" + "=" * 60)
    print("  LLaVA-NeXT-Video è§†é¢‘æ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    print("\næ­¤æµ‹è¯•ä½¿ç”¨ä¸“ç”¨è§†é¢‘ç†è§£æ¨¡å‹ LLaVA-NeXT-Video-7B")
    print("æ”¯æŒ 4-bit é‡åŒ–ï¼Œé€‚åˆ 12GB æ˜¾å­˜ GPU")

    # é€‰æ‹©é‡åŒ–æ¨¡å¼
    print("\né‡åŒ–é€‰é¡¹:")
    print("  [1] 4-bit é‡åŒ– (æ¨èï¼Œ~5GB VRAM)")
    print("  [2] FP16 å…¨ç²¾åº¦ (~14GB VRAMï¼Œå¯èƒ½OOM)")
    quant_choice = input("\né€‰æ‹© (é»˜è®¤1): ").strip() or "1"
    use_4bit = quant_choice == "1"

    # é…ç½®å‚æ•°
    num_frames = int(input("\né‡‡æ ·å¸§æ•° (æ¨è4-8ï¼Œé»˜è®¤8): ").strip() or "8")
    resolution = int(input("åˆ†è¾¨ç‡ (æ¨è224-336ï¼Œé»˜è®¤336): ").strip() or "336")

    print(f"\n{'='*60}")
    print(f"æµ‹è¯•é…ç½®")
    print(f"  è§†é¢‘: {video.name}")
    print(f"  æ¨¡å‹: LLaVA-NeXT-Video-7B {'(4-bit)' if use_4bit else '(FP16)'}")
    print(f"  é‡‡æ ·å¸§æ•°: {num_frames}")
    print(f"  åˆ†è¾¨ç‡: {resolution}x{resolution}")
    print(f"{'='*60}")

    confirm = input("\nç¡®è®¤å¼€å§‹? (y/n): ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        return

    # æ¸…ç†æ˜¾å­˜
    import gc
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    print(f"\næ¸…ç†åæ˜¾å­˜: {get_vram_usage():.2f}GB")

    # åŠ è½½æ¨¡å‹
    print("\næ­£åœ¨åŠ è½½ LLaVA-NeXT-Video-7B...")
    from models.vlm_loader import VLMLoader

    vlm = VLMLoader()
    model_key = "llava-next-video-7b-4bit" if use_4bit else "llava-next-video-7b"

    load_start = time.time()
    success = vlm.load_model(model_key)
    load_time = time.time() - load_start

    if not success:
        print("æ¨¡å‹åŠ è½½å¤±è´¥")
        return

    vram = get_vram_usage()
    print(f"æ¨¡å‹åŠ è½½å®Œæˆ: {load_time:.2f}s, æ˜¾å­˜: {vram:.2f}GB")

    result1 = ""
    result2 = ""
    infer_time1 = 0
    infer_time2 = 0

    try:
        # æµ‹è¯•1: åœºæ™¯å’ŒåŠ¨ä½œæè¿°
        print("\n" + "=" * 60)
        print("æµ‹è¯•1: è§†é¢‘å†…å®¹æè¿°")
        print("-" * 60)

        prompt1 = "Please describe this video in detail. What is happening? Describe the scene, people, and their actions."

        infer_start = time.time()
        result1 = vlm.generate_from_video(
            video_path=str(video),
            prompt=prompt1,
            num_frames=num_frames,
            max_new_tokens=512,
            temperature=0.7,
            resolution=resolution,
        )
        infer_time1 = time.time() - infer_start

        print(f"è€—æ—¶: {infer_time1:.2f}s")
        print(f"\n{result1}")

        torch.cuda.empty_cache()

        # æµ‹è¯•2: åŠ¨ä½œè¯†åˆ«
        print("\n" + "=" * 60)
        print("æµ‹è¯•2: åŠ¨ä½œè¯†åˆ«ä¸“é¡¹")
        print("-" * 60)

        prompt2 = "What activity or action is the person doing in this video? Is it a static pose or dynamic movement? If dynamic, describe the type of activity (e.g., dancing, exercising, cooking, working)."

        infer_start = time.time()
        result2 = vlm.generate_from_video(
            video_path=str(video),
            prompt=prompt2,
            num_frames=num_frames,
            max_new_tokens=256,
            temperature=0.5,
            resolution=resolution,
        )
        infer_time2 = time.time() - infer_start

        print(f"è€—æ—¶: {infer_time2:.2f}s")
        print(f"\n{result2}")

        # æ€§èƒ½ç»Ÿè®¡
        print("\n" + "=" * 60)
        print("æ€§èƒ½ç»Ÿè®¡")
        print("=" * 60)
        print(f"  æ¨¡å‹: LLaVA-NeXT-Video-7B {'(4-bit)' if use_4bit else '(FP16)'}")
        print(f"  æ¨¡å‹åŠ è½½: {load_time:.2f}s")
        print(f"  æµ‹è¯•1è€—æ—¶: {infer_time1:.2f}s")
        print(f"  æµ‹è¯•2è€—æ—¶: {infer_time2:.2f}s")
        print(f"  æ€»æ¨ç†æ—¶é—´: {infer_time1 + infer_time2:.2f}s")
        print(f"  æ˜¾å­˜å ç”¨: {get_vram_usage():.2f}GB")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # æ¸…ç†
        print("\næ­£åœ¨æ¸…ç†æ˜¾å­˜...")
        vlm.unload_model()
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        print(f"æ¸…ç†å®Œæˆï¼Œå½“å‰æ˜¾å­˜: {get_vram_usage():.2f}GB")

    # ä¿å­˜ç»“æœ
    if result1 or result2:
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = OUTPUT_DIR / f"llava_next_video_test_{video.name}_{timestamp_str}.json"

        result = {
            "video": video.name,
            "model": model_key,
            "quantization": "4bit" if use_4bit else "fp16",
            "num_frames": num_frames,
            "resolution": resolution,
            "load_time": load_time,
            "test1": {
                "prompt": "è§†é¢‘å†…å®¹æè¿°",
                "result": result1,
                "time": infer_time1,
            },
            "test2": {
                "prompt": "åŠ¨ä½œè¯†åˆ«",
                "result": result2,
                "time": infer_time2,
            },
            "vram_gb": vram,
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜: {output_path}")


# ==================== Pipeline ç³»ç»Ÿæµ‹è¯• ====================

def run_pipeline_benchmark():
    """Pipeline ç³»ç»Ÿçº§ benchmark - æµ‹è¯•å®Œæ•´æµç¨‹"""
    videos = list_test_videos()
    if not videos:
        return

    video = select_video(videos)
    if not video:
        return

    model_key = select_model()

    print("\n" + "=" * 60)
    print("  Pipeline ç³»ç»Ÿçº§ Benchmark")
    print("=" * 60)
    print("\næµ‹è¯•å®Œæ•´æµç¨‹: FrameBuffer â†’ HybridTrigger â†’ VideoAnalyzer")
    print("æ­¤æµ‹è¯•ä¼šå¿«é€Ÿå¤„ç†è§†é¢‘ï¼ˆéå®æ—¶ï¼‰ï¼Œè®°å½•æ¯æ¬¡åˆ†æçš„æ€§èƒ½æŒ‡æ ‡")

    # é…ç½®å‚æ•°
    print("\nå‚æ•°é…ç½®:")
    version_tag = input("  ç‰ˆæœ¬æ ‡è¯† (å¦‚ v1.0-baseline): ").strip() or "baseline"
    version_desc = input("  ç‰ˆæœ¬æè¿° (å¦‚ åˆå§‹åŸºå‡†): ").strip() or "æ— æè¿°"
    trigger_interval = float(input("  è§¦å‘é—´éš”ç§’æ•° (é»˜è®¤10): ").strip() or "10")
    max_analyses = input("  æœ€å¤§åˆ†ææ¬¡æ•° (å›è½¦=ä¸é™åˆ¶): ").strip()
    max_analyses = int(max_analyses) if max_analyses else None

    print(f"\n{'='*60}")
    print(f"æµ‹è¯•é…ç½®")
    print(f"  è§†é¢‘: {video.name}")
    print(f"  æ¨¡å‹: {model_key}")
    print(f"  è§¦å‘é—´éš”: {trigger_interval}ç§’")
    print(f"  æœ€å¤§åˆ†ææ¬¡æ•°: {max_analyses or 'ä¸é™åˆ¶'}")
    print(f"  GPU: {get_gpu_info()}")
    print(f"{'='*60}")

    confirm = input("\nç¡®è®¤å¼€å§‹? (y/n): ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        return

    import gc
    gc.collect()
    torch.cuda.empty_cache()

    # å¯¼å…¥ pipeline ç»„ä»¶
    from core.frame_buffer import FrameBuffer
    from core.hybrid_trigger import HybridTrigger
    from core.video_analyzer import VideoAnalyzer
    from utils.video_processor import VideoProcessor

    # åˆå§‹åŒ–ç»„ä»¶
    print("\nåˆå§‹åŒ– Pipeline ç»„ä»¶...")

    frame_buffer = FrameBuffer(max_frames=300, max_age_seconds=30.0)
    trigger = HybridTrigger(
        scan_interval=trigger_interval,
        motion_threshold=0.05,
        cooldown=2.0,
    )

    # åŠ è½½åˆ†æå™¨
    load_start = time.time()
    analyzer = VideoAnalyzer(model_key=model_key)
    load_time = time.time() - load_start
    vram_after_load = get_vram_usage()
    print(f"æ¨¡å‹åŠ è½½: {load_time:.2f}s, æ˜¾å­˜: {vram_after_load:.2f}GB")

    # æ‰“å¼€è§†é¢‘
    processor = VideoProcessor(str(video))
    video_duration = processor.video_info.duration
    fps = processor.video_info.fps
    print(f"è§†é¢‘æ—¶é•¿: {video_duration:.1f}s, FPS: {fps}")

    # æµ‹è¯•æ•°æ®æ”¶é›†
    analysis_results = []
    frame_count = 0
    analysis_count = 0

    print("\nå¼€å§‹å¤„ç†...")
    process_start = time.time()

    try:
        # æ¨¡æ‹Ÿé€å¸§å¤„ç†
        for frame_info in processor.extract_frames(interval=0.5):  # æ¯0.5ç§’å–ä¸€å¸§
            frame_count += 1
            timestamp = frame_info.timestamp

            # æ·»åŠ åˆ°ç¼“å­˜
            frame_buffer.add_frame(frame_info.image, timestamp)

            # æ£€æŸ¥è§¦å‘
            should_trigger, reason = trigger.check(
                frame=frame_info.image,
                current_time=timestamp,
            )

            if should_trigger:
                analysis_count += 1
                reason_str = reason.value if hasattr(reason, 'value') else str(reason)
                print(f"\n[åˆ†æ {analysis_count}] æ—¶é—´: {timestamp:.1f}s, åŸå› : {reason_str}")

                # ä»ç¼“å†²åŒºè·å–å¸§
                frames = frame_buffer.get_frames(count=6)
                if not frames:
                    print("  è·³è¿‡: ç¼“å†²åŒºæ— å¯ç”¨å¸§")
                    continue

                # æ‰§è¡Œåˆ†æ
                analysis_start = time.time()
                result = analyzer.analyze_now(frames=frames)
                analysis_time = time.time() - analysis_start

                # è®°å½•ç»“æœ
                analysis_results.append({
                    "index": analysis_count,
                    "timestamp": timestamp,
                    "trigger_reason": reason_str,
                    "success": result.success,
                    "analysis_time": round(analysis_time, 3),
                    "description": result.description[:200] if result.description else "",
                    "error": result.error or "",
                })

                status = "âœ“" if result.success else f"âœ— {result.error}"
                print(f"  è€—æ—¶: {analysis_time:.2f}s, çŠ¶æ€: {status}")

                if max_analyses and analysis_count >= max_analyses:
                    print(f"\nè¾¾åˆ°æœ€å¤§åˆ†ææ¬¡æ•° {max_analyses}")
                    break

    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­")

    finally:
        processor.close()

    process_time = time.time() - process_start

    # ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("  Benchmark ç»“æœ")
    print("=" * 60)

    successful = [r for r in analysis_results if r["success"]]
    failed = [r for r in analysis_results if not r["success"]]
    analysis_times = [r["analysis_time"] for r in successful]

    print(f"\nåŸºæœ¬ä¿¡æ¯:")
    print(f"  è§†é¢‘: {video.name}")
    print(f"  æ¨¡å‹: {model_key}")
    print(f"  GPU: {get_gpu_info()}")

    print(f"\nå¤„ç†ç»Ÿè®¡:")
    print(f"  è§†é¢‘æ—¶é•¿: {video_duration:.1f}s")
    print(f"  å®é™…å¤„ç†æ—¶é—´: {process_time:.1f}s")
    print(f"  å¤„ç†å¸§æ•°: {frame_count}")
    print(f"  åˆ†ææ¬¡æ•°: {len(analysis_results)}")
    print(f"  æˆåŠŸ: {len(successful)}, å¤±è´¥: {len(failed)}")
    print(f"  æˆåŠŸç‡: {len(successful)/len(analysis_results)*100:.1f}%" if analysis_results else "  æˆåŠŸç‡: N/A")

    if analysis_times:
        print(f"\nå»¶è¿Ÿç»Ÿè®¡ (æˆåŠŸçš„åˆ†æ):")
        print(f"  å¹³å‡: {sum(analysis_times)/len(analysis_times):.2f}s")
        print(f"  æœ€å°: {min(analysis_times):.2f}s")
        print(f"  æœ€å¤§: {max(analysis_times):.2f}s")
        # P95
        sorted_times = sorted(analysis_times)
        p95_idx = int(len(sorted_times) * 0.95)
        print(f"  P95: {sorted_times[p95_idx] if p95_idx < len(sorted_times) else sorted_times[-1]:.2f}s")

    print(f"\nèµ„æºä½¿ç”¨:")
    print(f"  æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.2f}s")
    print(f"  æ˜¾å­˜å ç”¨: {vram_after_load:.2f}GB")
    print(f"  å½“å‰æ˜¾å­˜: {get_vram_usage():.2f}GB")

    # ä¿å­˜æŠ¥å‘Š
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_version = version_tag.replace("/", "-").replace(" ", "_")
    output_path = OUTPUT_DIR / f"benchmark_{safe_version}_{video.name}_{timestamp_str}.json"

    report = {
        "type": "pipeline_benchmark",
        "version": version_tag,
        "description": version_desc,
        "test_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "config": {
            "video": video.name,
            "video_duration": video_duration,
            "model": model_key,
            "trigger_interval": trigger_interval,
            "gpu": get_gpu_info(),
        },
        "performance": {
            "model_load_time": round(load_time, 2),
            "vram_gb": round(vram_after_load, 2),
            "process_time": round(process_time, 2),
            "frames_processed": frame_count,
            "total_analyses": len(analysis_results),
            "successful_analyses": len(successful),
            "failed_analyses": len(failed),
            "success_rate": round(len(successful)/len(analysis_results)*100, 1) if analysis_results else 0,
        },
        "latency": {
            "avg": round(sum(analysis_times)/len(analysis_times), 3) if analysis_times else 0,
            "min": round(min(analysis_times), 3) if analysis_times else 0,
            "max": round(max(analysis_times), 3) if analysis_times else 0,
            "p95": round(sorted(analysis_times)[int(len(analysis_times)*0.95)] if analysis_times else 0, 3),
        },
        "analyses": analysis_results,
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {output_path}")

    # æ¸…ç†
    analyzer.close()
    gc.collect()
    torch.cuda.empty_cache()


# ==================== è§†é¢‘æ¨¡å‹å¯¹æ¯”æµ‹è¯• ====================

def run_video_model_comparison():
    """å¤šè§†é¢‘æ¨¡å‹å¯¹æ¯”æµ‹è¯•"""
    videos = list_test_videos()
    if not videos:
        return

    video = select_video(videos)
    if not video:
        return

    print("\n" + "=" * 60)
    print("  è§†é¢‘æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    print("\nå°†æµ‹è¯•ä»¥ä¸‹æ¨¡å‹ï¼ˆå‡ä½¿ç”¨4-bité‡åŒ–ï¼‰ï¼š")
    print("  1. Qwen2-VL-2B (è§†é¢‘æ¨¡å¼)")
    print("  2. LLaVA-NeXT-Video-7B-4bit")
    print("  3. Video-LLaVA-7B-4bit")

    # é…ç½®å‚æ•°
    num_frames = int(input("\né‡‡æ ·å¸§æ•° (æ¨è6-8ï¼Œé»˜è®¤8): ").strip() or "8")
    resolution = int(input("åˆ†è¾¨ç‡ (æ¨è336ï¼Œé»˜è®¤336): ").strip() or "336")

    print(f"\n{'='*60}")
    print(f"æµ‹è¯•é…ç½®")
    print(f"  è§†é¢‘: {video.name}")
    print(f"  é‡‡æ ·å¸§æ•°: {num_frames}")
    print(f"  åˆ†è¾¨ç‡: {resolution}x{resolution}")
    print(f"{'='*60}")

    confirm = input("\nç¡®è®¤å¼€å§‹? (y/n): ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        return

    # æµ‹è¯•æ¨¡å‹åˆ—è¡¨
    test_models = [
        ("qwen2-vl-2b", "Qwen2-VL-2B", True),  # (key, display_name, is_qwen)
        ("llava-next-video-7b-4bit", "LLaVA-NeXT-Video-7B", False),
        ("video-llava-7b-4bit", "Video-LLaVA-7B", False),
    ]

    # æµ‹è¯•æç¤ºè¯
    prompt_cn = "è¯·æè¿°è¿™æ®µè§†é¢‘çš„å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€äººç‰©å’Œä»–ä»¬æ­£åœ¨è¿›è¡Œçš„æ´»åŠ¨ã€‚"
    prompt_en = "Describe this video. What is the scene, who is in it, and what activity are they doing?"

    results = []
    import gc

    for model_key, display_name, is_qwen in test_models:
        print(f"\n{'#' * 60}")
        print(f"# æµ‹è¯•: {display_name}")
        print(f"{'#' * 60}")

        # æ¸…ç†æ˜¾å­˜
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

        from models.vlm_loader import VLMLoader
        vlm = VLMLoader()

        result = {
            "model": display_name,
            "model_key": model_key,
            "load_time": 0,
            "infer_time": 0,
            "vram_gb": 0,
            "response": "",
            "error": "",
        }

        try:
            # åŠ è½½æ¨¡å‹
            load_start = time.time()
            success = vlm.load_model(model_key)
            result["load_time"] = time.time() - load_start

            if not success:
                result["error"] = "æ¨¡å‹åŠ è½½å¤±è´¥"
                results.append(result)
                continue

            result["vram_gb"] = get_vram_usage()
            print(f"åŠ è½½å®Œæˆ: {result['load_time']:.2f}s, æ˜¾å­˜: {result['vram_gb']:.2f}GB")

            # æ¨ç†
            prompt = prompt_cn if is_qwen else prompt_en
            infer_start = time.time()
            response = vlm.generate_from_video(
                video_path=str(video),
                prompt=prompt,
                num_frames=num_frames,
                max_new_tokens=256,
                temperature=0.5,
                resolution=resolution,
            )
            result["infer_time"] = time.time() - infer_start
            result["response"] = response

            print(f"æ¨ç†è€—æ—¶: {result['infer_time']:.2f}s")
            print(f"\nå›å¤:\n{response[:300]}{'...' if len(response) > 300 else ''}")

        except Exception as e:
            result["error"] = str(e)
            print(f"é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

        finally:
            vlm.unload_model()
            gc.collect()
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

        results.append(result)

    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print("\n" + "=" * 80)
    print("  æ¨¡å‹å¯¹æ¯”ç»“æœ")
    print("=" * 80)
    print(f"\n{'æ¨¡å‹':<25} {'åŠ è½½æ—¶é—´':>10} {'æ¨ç†æ—¶é—´':>10} {'æ˜¾å­˜':>10} {'çŠ¶æ€':<10}")
    print("-" * 80)

    for r in results:
        if r["error"]:
            status = f"âŒ {r['error'][:15]}"
            print(f"{r['model']:<25} {'-':>10} {'-':>10} {'-':>10} {status}")
        else:
            status = "âœ…"
            print(f"{r['model']:<25} {r['load_time']:>8.2f}s {r['infer_time']:>8.2f}s {r['vram_gb']:>8.2f}GB {status}")

    print("-" * 80)

    # è¯¦ç»†å›å¤å¯¹æ¯”
    print("\n" + "=" * 80)
    print("  å›å¤å†…å®¹å¯¹æ¯”")
    print("=" * 80)

    for r in results:
        if not r["error"]:
            print(f"\nã€{r['model']}ã€‘")
            print("-" * 40)
            print(r["response"])

    # ä¿å­˜ç»“æœ
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = OUTPUT_DIR / f"video_model_comparison_{video.name}_{timestamp_str}.json"

    report = {
        "video": video.name,
        "num_frames": num_frames,
        "resolution": resolution,
        "test_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "results": results,
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜: {output_path}")


# ==================== è®¾å¤‡æ ¡å‡† ====================

def run_device_calibration():
    """è®¾å¤‡æ€§èƒ½æ ¡å‡† - é¦–æ¬¡è¿è¡Œæˆ–æ¢è®¾å¤‡æ—¶æ‰§è¡Œ"""
    print("\n" + "=" * 60)
    print("  è®¾å¤‡æ€§èƒ½æ ¡å‡† (Calibration)")
    print("=" * 60)
    print("\næ­¤å·¥å…·ä¼šæµ‹è¯•å½“å‰è®¾å¤‡çš„æ¨ç†æ€§èƒ½ï¼Œç”Ÿæˆå„çº§åˆ«æœ€ä¼˜é…ç½®")
    print("æ­¥éª¤:")
    print("  1. æµ‹è¯• Flash Attention vs Eager Attention")
    print("  2. ä½¿ç”¨æ›´ä¼˜çš„ Attention è¿è¡Œå®Œæ•´æ€§èƒ½æµ‹è¯•")
    print("\né¢„è®¡è€—æ—¶: 5-15 åˆ†é’Ÿ (å–å†³äº GPU æ€§èƒ½)")

    # é€‰æ‹©æ¨¡å‹
    print("\nå¯ç”¨æ¨¡å‹:")
    models = [
        ("Qwen/Qwen2-VL-2B-Instruct", "Qwen2-VL-2B (æ¨è)"),
        ("Qwen/Qwen2-VL-7B-Instruct", "Qwen2-VL-7B"),
    ]
    for i, (_, desc) in enumerate(models, 1):
        print(f"  [{i}] {desc}")

    model_choice = input("\né€‰æ‹©æ¨¡å‹ (é»˜è®¤1): ").strip() or "1"
    try:
        model_idx = int(model_choice) - 1
        model_name = models[model_idx][0]
    except (ValueError, IndexError):
        model_name = models[0][0]

    print(f"\né€‰æ‹©çš„æ¨¡å‹: {model_name}")
    print(f"GPU: {get_gpu_info()}")

    confirm = input("\nç¡®è®¤å¼€å§‹æ ¡å‡†? (y/n): ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        return

    # è¿è¡Œæ ¡å‡†
    from core.adaptive_config import AdaptiveConfig

    config = AdaptiveConfig(model_name=model_name)
    profile = config.calibrate(verbose=True)

    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 60)
    print("  æ ¡å‡†ç»“æœ")
    print("=" * 60)

    print(f"\né…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {config.profile_path}")

    # Flash vs Eager ç»“æœ
    fve = profile.flash_vs_eager
    print(f"\nFlash vs Eager å¯¹æ¯” ({fve['test_config']}):")
    if fve.get("flash_available"):
        print(f"  Flash:  {fve['flash_time']:.2f}s, {fve['flash_vram']:.2f}GB")
        print(f"  Eager:  {fve['eager_time']:.2f}s, {fve['eager_vram']:.2f}GB")
        print(f"  Flash åŠ é€Ÿ: {fve['flash_speedup_pct']:.1f}%")
    else:
        print(f"  Flash Attention ä¸å¯ç”¨")
        print(f"  Eager: {fve['eager_time']:.2f}s, {fve['eager_vram']:.2f}GB")

    print(f"\né€‰ç”¨: {'Flash Attention 2' if profile.use_flash_attention else 'Eager Attention'}")

    print("\nå¯ç”¨çš„å®æ—¶æ€§çº§åˆ«:")
    for level in ["fast", "balanced", "thorough"]:
        cfg = profile.computed_configs[level]
        print(f"\n  ã€{level}ã€‘")
        print(f"    å‘¨æœŸ: {cfg['cycle_seconds']}ç§’")
        print(f"    æ”¶é›†æ—¶é—´: {cfg['collect_seconds']:.1f}ç§’")
        print(f"    åˆ†ææ—¶é—´: {cfg['analysis_seconds']:.1f}ç§’")
        print(f"    å¸§æ•°: {cfg['frames']}")
        print(f"    åˆ†è¾¨ç‡: {cfg['resolution']}px")
        print(f"    é‡‡æ ·é—´éš”: {cfg['sample_interval']:.2f}ç§’/å¸§")

    print("\n" + "-" * 60)
    print("ä½¿ç”¨æ–¹å¼:")
    print("  from core.adaptive_config import get_adaptive_config")
    print("  config = get_adaptive_config('balanced')")
    print("-" * 60)


# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»èœå•"""
    print("\n" + "=" * 60)
    print("  è§†é¢‘ç›‘æ§è§‚å¯Ÿå±‚ - æµ‹è¯•å·¥å…·")
    print("=" * 60)

    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("  [1] å•æ¨¡å‹æµ‹è¯•")
        print("  [2] å¤šæ¨¡å‹å¯¹æ¯” (å«æ€§èƒ½ç»Ÿè®¡)")
        print("  [3] å¤šå¸§åŠ¨æ€è¯†åˆ«æµ‹è¯•")
        print("  [4] Qwen2-VL è§†é¢‘æ¨¡å¼æµ‹è¯•")
        print("  [5] LLaVA-NeXT-Video æµ‹è¯• (4-bit)")
        print("  [6] è§†é¢‘æ¨¡å‹å¯¹æ¯”æµ‹è¯• (3æ¨¡å‹)")
        print("  [7] Pipeline ç³»ç»Ÿçº§ Benchmark â­")
        print("  [8] åˆ—å‡ºæµ‹è¯•è§†é¢‘")
        print("  [9] è®¾å¤‡æ€§èƒ½æ ¡å‡† (Calibration) â­")
        print("  [q] é€€å‡º")

        choice = input("\né€‰æ‹©: ").strip().lower()

        if choice == "1":
            run_single_model_test()
        elif choice == "2":
            run_model_comparison()
        elif choice == "3":
            run_multi_frame_test()
        elif choice == "4":
            run_video_mode_test()
        elif choice == "5":
            run_llava_next_video_test()
        elif choice == "6":
            run_video_model_comparison()
        elif choice == "7":
            run_pipeline_benchmark()
        elif choice == "8":
            list_test_videos()
        elif choice == "9":
            run_device_calibration()
        elif choice == "q":
            print("å†è§!")
            break
        else:
            print("æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    main()
